{"cells":[{"source":["#%matplotlib inline\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["import tensorflow as tf\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["sess = tf.InteractiveSession()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["import keras.backend as K\n","K.set_image_data_format(\"channels_first\")\n","\n","import keras\n","import numpy as np\n","\n","from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D\n","from keras.layers import Flatten, Lambda, BatchNormalization\n","from keras.models import Sequential\n","from keras.optimizers import Adam as Adam\n","from keras.layers.advanced_activations import LeakyReLU\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Used to save and load training histories\n","import pickle\n","from collections import defaultdict\n","\n","import resource, sys\n","\n","# limit recursion depth\n","limit = sys.getrecursionlimit()\n","print(limit)\n","\n","#resource.setrlimit(resource.RLIMIT_STACK, (2**29, -1))\n","#sys.setrecusionlimit(2**29 - 1)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["from scipy import io as spio\n","emnist = spio.loadmat(\"/Users/andereggt/datasets/emnist/matlab/emnist-digits.mat\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Load training dataset and labels\n","x_train = emnist[\"dataset\"][0][0][0][0][0][0]\n","x_train = x_train.astype(np.float32)\n","\n","y_train = emnist[\"dataset\"][0][0][0][0][0][1]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Load test dataset and labels\n","x_test = emnist[\"dataset\"][0][0][1][0][0][0]\n","x_test = x_test.astype(np.float32)\n","\n","y_test = emnist[\"dataset\"][0][0][1][0][0][1]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Store labels for visualization\n","train_labels = y_train\n","test_labels = y_test\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print(\"Training data shape: \", x_train.shape)\n","print(\"Training label shape: \", y_train.shape)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Normalize datasets\n","x_train /= 255\n","x_test /= 255\n","print(x_train)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Reshape using matlab order\n","x_train = x_train.reshape(x_train.shape[0], 1, 28, 28, order=\"A\")\n","x_test = x_test.reshape(x_test.shape[0], 1, 28, 28, order=\"A\")\n","\n","print(\"Reshaped training data: \", x_train.shape)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# labels should be onehot encoded\n","y_train = keras.utils.to_categorical(y_train, 10)\n","y_test = keras.utils.to_categorical(y_test, 10)\n","\n","print(\"One-hot encoded label shape: \", y_train.shape)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Verify data has been loaded correctly\n","samplenum = 5437\n","\n","import matplotlib.pyplot as plt\n","img = x_train[samplenum]\n","plt.imshow(img[0], cmap='gray')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Reshape test labels\n","test_labels = test_labels.reshape(40000)\n","print(\"Reshaped test labels: \", test_labels.shape)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Define Model\n","# Calculate mean and standard deviation\n","mean_px = x_train.mean().astype(np.float32)\n","std_px = x_train.std().astype(np.float32)\n","\n","# Define function to normalize input data\n","def norm_input(x): return (x-mean_px)/std_px\n","\n","# Batchnorm + dropout + data augmentation\n","def create_model():\n","    model = Sequential([\n","        Lambda(norm_input, input_shape=(1,28,28), output_shape=(1,28,28)),\n","        Conv2D(32, (3,3)),\n","        LeakyReLU(),\n","        BatchNormalization(axis=1),\n","        Conv2D(32, (3,3)),\n","        LeakyReLU(),\n","        MaxPooling2D(),\n","        BatchNormalization(axis=1),\n","        Conv2D(64, (3,3)),\n","        LeakyReLU(),\n","        BatchNormalization(axis=1),\n","        Conv2D(64, (3,3)),\n","        LeakyReLU(),\n","        MaxPooling2D(),\n","        Flatten(),\n","        BatchNormalization(),\n","        Dense(512),\n","        LeakyReLU(),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","        Dense(10, activation='softmax')\n","    ])\n","    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Use Keras data augmentation\n","batch_size = 512\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","gen = ImageDataGenerator(rotation_range=12, width_shift_range=0.1, shear_range=0.3,\n","                         height_shift_range=0.1, zoom_range=0.1, data_format='channels_first')\n","batches = gen.flow(x_train, y_train, batch_size=batch_size)\n","test_batches = gen.flow(x_test, y_test, batch_size=batch_size)\n","steps_per_epoch = int(np.ceil(batches.n/batch_size))\n","validation_steps = int(np.ceil(test_batches.n/batch_size))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Visualize data gen\n","import matplotlib.pyplot as plt\n","img = x_train[1]\n","plt.imshow(img[0], cmap='gray')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Get augmented images\n","img = np.expand_dims(img, axis=0)\n","aug_iter = gen.flow(img)\n","\n","aug_img = next(aug_iter)[0].astype(np.float32)\n","print(\"Augmented image shape: \", aug_img.shape)\n","\n","import matplotlib.pyplot as plt\n","\n","f = plt.figure(figsize=(12,6))\n","for i in range(8):\n","    sp = f.add_subplot(2, 26//3, i+1)\n","    sp.axis('Off')\n","    aug_img = next(aug_iter)[0].astype(np.float32)\n","    plt.imshow(aug_img[0], cmap='gray')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Create 10 models\n","models = []\n","weights_epoch = 0\n","\n","for i in range(10):\n","    m = create_model()\n","    models.append(m)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["eval_batch_size = 512\n","num_iterations = 1\n","num_epochs = 10\n","\n","import os\n","if not os.path.exists(\"dropout_0.2\"):\n","    os.mkdir(\"dropout_0.2\")\n","if not os.path.exists(\"dropout_0.2/weights\"):\n","    os.mkdir(\"dropout_0.2/weights\")\n","if not os.path.exists(\"dropout_0.2/history\"):\n","    os.mkdir(\"dropout_0.2/history\")\n","\n","for iteration in range(num_iterations):\n","    cur_epoch = (iteration + 1) * num_epochs + weights_epoch\n","\n","    for i, m in enumerate(models):\n","        m.optimizer.lr = 0.000001\n","        h = m.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=0,\n","                            validation_data=test_batches, validation_steps=validation_steps)\n","        m.save_weights(\"dropout_0.2/weights/{:03d}epochs_weights_model_{}.pkl\".format(cur_epoch, i))\n","    \n","    # evaluate test error rate for ensemble\n","    all_preds = np.stack([m.predict(x_test, batch_size=eval_batch_size) for m in models])\n","    avg_preds = all_preds.mean(axis=0)\n","    test_error_ensemble = (1 - keras.metrics.categorical_accuracy(y_test, avg_preds).eval().mean()) * 100\n","\n","    # write test error rate for ensemble and every single model to text file\n","    with open(\"dropout_0.2/history/test_errors_epoch_{:03d}.txt\".format(cur_epoch), \"w\") as text_file:\n","        text_file.write(\"epoch: {} test error on ensemble: {}\\n\".format(cur_epoch, test_error_ensemble))\n","        \n","        for m in models:\n","            pred = np.array(m.predict(x_test, batch_size=eval_batch_size))\n","            test_err = (1 - keras.metrics.categorical_accuracy(y_test, pred).eval().mean()) * 100\n","            text_file.write(\"{}\\n\".format(test_err))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["eval_batch_size = 512\n","all_preds = np.stack([m.predict(x_test, batch_size=eval_batch_size) for m in models])\n","avg_preds = all_preds.mean(axis=0)\n","print(\"Ensemble error rate: \", (1 - keras.metrics.categorical_accuracy(y_test, avg_preds).eval().mean()) * 100)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}